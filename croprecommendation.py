# -*- coding: utf-8 -*-
"""croprecommendation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OAjS37vJTSFExPbA0trW8dEe9guWGmru
"""

!pip install --pre pycaret

import pandas as pd

df=pd.read_csv('Crop_recommendation.csv')

df

from sklearn.model_selection import train_test_split

training_data, testing_data = train_test_split(df, test_size=0.4, random_state=0)

print(f"No. of training examples: {training_data.shape[0]}")
print(f"No. of testing examples: {testing_data.shape[0]}")

testing_data

from pycaret.classification import *

s1=setup(data=training_data,target='label')

best1=compare_models()

print(best1)

finalize_model(best1)

evaluate_model(best1)

predict_model(best1)

predict_model(best1,testing_data)

qda=create_model('qda')

nb=create_model('nb')

rf=create_model('rf')

et=create_model('et')

import pandas as pd
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score

X=df[['N',	'P',	'K',	'temperature',	'humidity',	'ph',	'rainfall']]
y=df['label']

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=0)

clf1=GaussianNB()
 clf1.fit(X_train,y_train)

y_pred=clf1.predict(X_test)
print(classification_report(y_test,y_pred))

print(accuracy_score(y_test,y_pred))
print(precision_score(y_test,y_pred,average='macro'))
print(recall_score(y_test,y_pred,average='macro'))
print(f1_score(y_test,y_pred,average='macro'))

clf1.predict([[90,	42,	43,	20.879744,	82.002744,	6.502985	,202.935536	]])

from sklearn.ensemble import RandomForestClassifier

clf2=RandomForestClassifier()
clf2.fit(X_train,y_train)

y_pred_rf=clf2.predict(X_test)
print(classification_report(y_test,y_pred_rf))

print(accuracy_score(y_test,y_pred_rf))
print(precision_score(y_test,y_pred_rf,average='macro'))
print(recall_score(y_test,y_pred_rf,average='macro'))
print(f1_score(y_test,y_pred_rf,average='macro'))

clf2.predict([[67	,43,	39,	26.043720	,84.969070,	5.999969	,186.753677]])

from sklearn.ensemble import ExtraTreesClassifier

clf3=ExtraTreesClassifier()
clf3.fit(X_train,y_train)

y_pred_et=clf3.predict(X_test)
print(classification_report(y_test,y_pred_et))

print(accuracy_score(y_test,y_pred_et))
print(precision_score(y_test,y_pred_et,average='macro'))
print(recall_score(y_test,y_pred_et,average='macro'))
print(f1_score(y_test,y_pred_et,average='macro'))

clf3.predict([[67	,43,	39,	26.043720	,84.969070,	5.999969	,186.753677]])

from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
clf4=QuadraticDiscriminantAnalysis()
clf4.fit(X_train,y_train)

y_pred_qda=clf4.predict(X_test)
print(classification_report(y_test,y_pred_qda))

print(accuracy_score(y_test,y_pred_qda))
print(precision_score(y_test,y_pred_qda,average='macro'))
print(recall_score(y_test,y_pred_qda,average='macro'))
print(f1_score(y_test,y_pred_qda,average='macro'))

clf4.predict([[90,	42,	43,	20.879744,	82.002744,	6.502985	,202.935536	]])

from sklearn.ensemble import VotingClassifier
voting_classifier_hard_et_qda=VotingClassifier(estimators=[('clf3',ExtraTreesClassifier()),('clf4',QuadraticDiscriminantAnalysis())],voting='hard')
voting_classifier_hard_rf_nb=VotingClassifier(estimators=[('clf1',GaussianNB()),('clf2',RandomForestClassifier())],voting='hard')
voting_classifier_hard_rf_et=VotingClassifier(estimators=[('clf2',RandomForestClassifier()),('clf3',ExtraTreesClassifier())],voting='hard')
voting_classifier_hard_rf_qda=VotingClassifier(estimators=[('clf2',RandomForestClassifier()),('clf4',QuadraticDiscriminantAnalysis())],voting='hard')
voting_classifier_hard_nb_et=VotingClassifier(estimators=[('clf1',GaussianNB()),('clf3',ExtraTreesClassifier())],voting='hard')
voting_classifier_hard_nb_qda=VotingClassifier(estimators=[('clf1',GaussianNB()),('clf4',QuadraticDiscriminantAnalysis())],voting='hard')

voting_classifier_soft_et_qda=VotingClassifier(estimators=[('clf3',ExtraTreesClassifier()),('clf4',QuadraticDiscriminantAnalysis())],voting='soft')
voting_classifier_soft_rf_nb=VotingClassifier(estimators=[('clf1',GaussianNB()),('clf2',RandomForestClassifier())],voting='soft')
voting_classifier_soft_rf_et=VotingClassifier(estimators=[('clf2',RandomForestClassifier()),('clf3',ExtraTreesClassifier())],voting='soft')
voting_classifier_soft_rf_qda=VotingClassifier(estimators=[('clf2',RandomForestClassifier()),('clf4',QuadraticDiscriminantAnalysis())],voting='soft')
voting_classifier_soft_nb_et=VotingClassifier(estimators=[('clf1',GaussianNB()),('clf3',ExtraTreesClassifier())],voting='soft')
voting_classifier_soft_nb_qda=VotingClassifier(estimators=[('clf1',GaussianNB()),('clf4',QuadraticDiscriminantAnalysis())],voting='soft')

voting_classifier_hard_et_qda.fit(X_train, y_train)
y_pred_vch_et_qda = voting_classifier_hard_et_qda.predict(X_test)
voting_classifier_hard_rf_nb.fit(X_train, y_train)
y_pred_vch_rf_nb = voting_classifier_hard_rf_nb.predict(X_test)
voting_classifier_hard_rf_et.fit(X_train, y_train)
y_pred_vch_rf_et = voting_classifier_hard_rf_et.predict(X_test)
voting_classifier_hard_rf_qda.fit(X_train, y_train)
y_pred_vch_rf_qda = voting_classifier_hard_rf_qda.predict(X_test)
voting_classifier_hard_nb_et.fit(X_train, y_train)
y_pred_vch_nb_et = voting_classifier_hard_nb_et.predict(X_test)
voting_classifier_hard_nb_qda.fit(X_train, y_train)
y_pred_vch_nb_qda = voting_classifier_hard_nb_qda.predict(X_test)

voting_classifier_soft_et_qda.fit(X_train, y_train)
y_pred_vcs_et_qda = voting_classifier_soft_et_qda.predict(X_test)
voting_classifier_soft_rf_nb.fit(X_train, y_train)
y_pred_vcs_rf_nb = voting_classifier_soft_rf_nb.predict(X_test)
voting_classifier_soft_rf_et.fit(X_train, y_train)
y_pred_vcs_rf_et = voting_classifier_soft_rf_et.predict(X_test)
voting_classifier_soft_rf_qda.fit(X_train, y_train)
y_pred_vcs_rf_qda = voting_classifier_soft_rf_qda.predict(X_test)
voting_classifier_soft_nb_et.fit(X_train, y_train)
y_pred_vcs_nb_et = voting_classifier_soft_nb_et.predict(X_test)
voting_classifier_soft_nb_qda.fit(X_train, y_train)
y_pred_vcs_nb_qda = voting_classifier_soft_nb_qda.predict(X_test)

#extra trees qda
print(accuracy_score(y_test, y_pred_vch_et_qda))
print(precision_score(y_test,y_pred_vch_et_qda,average='macro'))
print(recall_score(y_test,y_pred_vch_et_qda,average='macro'))
print(f1_score(y_test,y_pred_vch_et_qda,average='macro'))

#rf nb
print(accuracy_score(y_test, y_pred_vch_rf_nb))
print(precision_score(y_test,y_pred_vch_rf_nb,average='macro'))
print(recall_score(y_test,y_pred_vch_rf_nb,average='macro'))
print(f1_score(y_test,y_pred_vch_rf_nb,average='macro'))

#rf et
print(accuracy_score(y_test, y_pred_vch_rf_et))
print(precision_score(y_test,y_pred_vch_rf_et,average='macro'))
print(recall_score(y_test,y_pred_vch_rf_et,average='macro'))
print(f1_score(y_test,y_pred_vch_rf_et,average='macro'))

#rf qda
print(accuracy_score(y_test, y_pred_vch_rf_qda))
print(precision_score(y_test,y_pred_vch_rf_qda,average='macro'))
print(recall_score(y_test,y_pred_vch_rf_qda,average='macro'))
print(f1_score(y_test,y_pred_vch_rf_qda,average='macro'))

#nb et
print(accuracy_score(y_test, y_pred_vch_nb_et))
print(precision_score(y_test,y_pred_vch_nb_et,average='macro'))
print(recall_score(y_test,y_pred_vch_nb_et,average='macro'))
print(f1_score(y_test,y_pred_vch_nb_et,average='macro'))

#nb qda
print(accuracy_score(y_test, y_pred_vch_nb_qda))
print(precision_score(y_test,y_pred_vch_nb_qda,average='macro'))
print(recall_score(y_test,y_pred_vch_nb_qda,average='macro'))
print(f1_score(y_test,y_pred_vch_nb_qda,average='macro'))

#extra trees qda
print(accuracy_score(y_test, y_pred_vcs_et_qda))
print(precision_score(y_test,y_pred_vcs_et_qda,average='macro'))
print(recall_score(y_test,y_pred_vcs_et_qda,average='macro'))
print(f1_score(y_test,y_pred_vcs_et_qda,average='macro'))

#rf nb
print(accuracy_score(y_test, y_pred_vcs_rf_nb))
print(precision_score(y_test,y_pred_vcs_rf_nb,average='macro'))
print(recall_score(y_test,y_pred_vcs_rf_nb,average='macro'))
print(f1_score(y_test,y_pred_vcs_rf_nb,average='macro'))

#rf et
print(accuracy_score(y_test, y_pred_vcs_rf_et))
print(precision_score(y_test,y_pred_vcs_rf_et,average='macro'))
print(recall_score(y_test,y_pred_vcs_rf_et,average='macro'))
print(f1_score(y_test,y_pred_vcs_rf_et,average='macro'))

#rf qda
print(accuracy_score(y_test, y_pred_vcs_rf_qda))
print(precision_score(y_test,y_pred_vcs_rf_qda,average='macro'))
print(recall_score(y_test,y_pred_vcs_rf_qda,average='macro'))
print(f1_score(y_test,y_pred,average='macro'))

#nb et
print(accuracy_score(y_test, y_pred_vcs_nb_et))
print(precision_score(y_test,y_pred_vcs_nb_et,average='macro'))
print(recall_score(y_test,y_pred_vcs_nb_et,average='macro'))
print(f1_score(y_test,y_pred_vcs_nb_et,average='macro'))

#nb qda
print(accuracy_score(y_test, y_pred_vcs_nb_qda))
print(precision_score(y_test,y_pred_vcs_nb_qda,average='macro'))
print(recall_score(y_test,y_pred_vcs_nb_qda,average='macro'))
print(f1_score(y_test,y_pred_vcs_nb_qda,average='macro'))

# from sklearn.preprocessing import LabelBinarizer
# import matplotlib.pyplot as plt
# import seaborn as sns
# label_binarizer = LabelBinarizer().fit(y_train)
# y_onehot_test = label_binarizer.transform(y_test)
# y_onehot_test.shape  # (n_samples, n_classes)

# def roc_auc_plot(y_test, y_proba, label=' ', l='-', lw=1.0):
#     from sklearn.metrics import roc_curve, roc_auc_score
#     fpr, tpr, _ = roc_curve(y_test, y_proba[:,1])
#     ax.plot(fpr, tpr, linestyle=l, linewidth=lw,
#             label="%s (area=%.3f)"%(label,roc_auc_score(y_test, y_proba[:,1])))

# f, ax = plt.subplots(figsize=(12,8))

# roc_auc_plot(y_test,clf1.predict_proba(X_test),label=' Naive Bayes',l='-')
# roc_auc_plot(y_test,clf2.predict_proba(X_test),label='Random Forest Classifier ',l='-')
# roc_auc_plot(y_test,clf3.predict_proba(X_test),label='Extra Tree Classifier ',l='-')
# roc_auc_plot(y_test,clf4.predict_proba(X_test),label='QDA',l='-')

# ax.plot([0,1], [0,1], color='k', linewidth=0.5, linestyle='--',
#         )
# ax.legend(loc="lower right")
# ax.set_xlabel('False Positive Rate')
# ax.set_ylabel('True Positive Rate')
# ax.set_xlim([0, 1])
# ax.set_ylim([0, 1])
# ax.set_title('Receiver Operator Characteristic curves')
# sns.despine()

# from sklearn.metrics import roc_curve, auc
# from sklearn.multiclass import OneVsRestClassifier
# from sklearn.preprocessing import label_binarize
# import matplotlib.pyplot as plt

# y = label_binarize(y, classes=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21])
# n_classes = 22
# X_train, X_test, y_train, y_test =\
#     train_test_split(X, y, test_size=0.33, random_state=0)

# # classifier
# # roc1= OneVsRestClassifier(ExtraTreesClassifier())
# # roc_auc_plot(y_test,clf1.predict_proba(X_test),label=' Naive Bayes',l='-')
# # roc_auc_plot(y_test,clf2.predict_proba(X_test),label='Random Forest Classifier ',l='-')
# # roc_auc_plot(y_test,clf3.predict_proba(X_test),label='Extra Tree Classifier ',l='-')
# # roc_auc_plot(y_test,clf4.predict_proba(X_test),label='QDA',l='-')
# roc1 = OneVsRestClassifier(ExtraTreesClassifier())
# y_score = roc1.fit(X_train, y_train).decision_function(X_test)

# # Compute ROC curve and ROC area for each class
# fpr = dict()
# tpr = dict()
# roc_auc = dict()
# for i in range(n_classes):
#     fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])
#     roc_auc[i] = auc(fpr[i], tpr[i])

# # Plot of a ROC curve for a specific class
# for i in range(n_classes):
#     plt.figure()
#     plt.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f)' % roc_auc[i])
#     plt.plot([0, 1], [0, 1], 'k--')
#     plt.xlim([0.0, 1.0])
#     plt.ylim([0.0, 1.05])
#     plt.xlabel('False Positive Rate')
#     plt.ylabel('True Positive Rate')
#     plt.title('Receiver operating characteristic example')
#     plt.legend(loc="lower right")
#     plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
labelss=pd.Categorical(df['label'])
labelss

df['label'].value_counts()

df['label'].value_counts().plot.bar()

df.iloc[:,:7].describe().transpose()

df.corr()

sns.heatmap(df.corr(),cmap='coolwarm')

from sklearn.preprocessing import LabelEncoder
label_encoder=LabelEncoder()
label_encoder.fit(y)
y=label_encoder.transform(y)
classes=label_encoder.classes_

from sklearn.preprocessing imp nort MinMaxScaler
min_max_scaler=MinMaxScaler()
X_train_norm=min_max_scaler.fit_transform(X_train)
X_test_norm=min_max_scaler.fit_transform(X_test)

X_train_norm[0,0]

X_train.iloc[0,0]

from sklearn.multiclass import OneVsRestClassifier
from sklearn.metrics import roc_curve,auc

RF=OneVsRestClassifier(voting_classifier_soft_rf_qda)
RF.fit(X_train_norm,y_train)
y_pred_norm=RF.predict(X_test_norm)
pred_prob=RF.predict_proba(X_test_norm)

from sklearn.preprocessing import label_binarize
import numpy as np
y_test_binarized=label_binarize(y_test,classes=np.unique(y_test))
fpr={}
tpr={}
thresh={}
roc_auc=dict()
n_class=classes.shape[0]
for i in range(n_class):
    fpr[i],tpr[i],thresh[i]=roc_curve(y_test_binarized[:,i],pred_prob[:,i])
    roc_auc[i]=auc(fpr[i],tpr[i])
    plt.plot(fpr[i],tpr[i],linestyle='--',label='%s vs Rest (AUC=%0.2f)'%(classes[i],roc_auc[i]))
plt.plot([0,1],[0,1],'b--')
plt.xlim([0,1])
plt.ylim([0,1])
plt.title('Multiclass ROC curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='lower right')
plt.show()